# The model was trained using Google Colab's GPU

## Data was loaded in batches of size 32 with some data augumentation methods being applied

## Model consisted of an pretrained Xception base, a Flattening layer, 1 Dense layer with relu activation and 128 neurons, 0.5 DropOut layer and an output layer with 102 neurons and softmax activation 

## Model was trained using EarlyStopping, ReduceOnPlateau and ModelCheckpoint

## [Competition website](https://www.kaggle.com/c/ai6-dl-cohort-6-challenge)

## [My Kaggle Profile](https://www.kaggle.com/chizurumolorondu)
